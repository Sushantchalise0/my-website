<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to Setup Hadoop Cluster and Configure on AWS</title>
</head>
<body>
    <h1>How to Setup Hadoop Cluster and Configure on AWS</h1>
    <p><strong>By:</strong> Sushant Chalise</p>
    <p><strong>Guidance from:</strong> Asst. Prof. Basanta Joshi P.hD</p>

    <h2>Introduction</h2>
    <p>Apache Hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly. In this setup, we will take a single namenode and three datanode.</p>

    <h2>Pre-Requisites</h2>
    <p>We will need an AWS account (here I will be using the Sandbox Environment of the AWS Academy). For the terminal access, we will be using MobaXterm, an application that provides X-Server capability for the Microsoft Windows operating system with SSH client and more. The shell environment can be downloaded here.</p>

    <h2>Before we start</h2>
    <p>In this article, many pieces of information such as public keys, private keys, Domain Names, and IP addresses are exposed. Different consequences must be considered before exposing these details. This article is solely based on educational purposes and the Instance created during this phase has been deleted.</p>

    <h2>AWS EC2 Startup</h2>
    <p>We start the Sandbox environment by clicking on the ‘Start Lab’. Also, we need to note that being the free tier, we are restricted to use the environment for a few hours, and also there are certain restrictions to use the ‘paid’ type of Instances.</p>
    <p>The main aim is to create a single namenode and three datanode. For this setup, we will initially create an Instance and set up the instance with common configuration (common configuration meaning all the configurations that are the same for namenode and datanode). Then we will replicate the instance (mirror it for 3) and finally set up individual instances.</p>

    <h3>Creating the Instance</h3>
    <p>To create an Instance we use the service EC2 and press launch instance.</p>
    <p>And use the following configuration:</p>
    <ul>
        <li>Name: hadoop</li>
        <li>Application and OS Image: ubuntu (24.04)</li>
        <li>Instance Type: t2.micro (we choose t2.micro since that is only available for the free tier. If you have a need for a high-memory or high-cpu instance, you can select one of those.)</li>
        <li>Keypair: Click ‘Create a new key pair’ and give it a name. Once downloaded, select the key generated.</li>
    </ul>
    <p><strong>Note:</strong> To connect to the instance with the MobaXterm securely, we will need to generate a key pair. The key pair consists of primary and secondary keys. To generate a key pair click on ‘Generate a new key pair’ it will create a dialogue box of where to save it, give the key pair the name and save it. For this, I will be using the name hadoop and saving it into the default Downloads folder. Keep all the other configurations the same and launch the instance.</p>

    <h3>Setting Up Instance</h3>
    <p>Once the instances are up and running, it is time to set them up for our purpose. This includes the following:</p>
    <ul>
        <li>Setup password-less login between the namenode and the datanodes.</li>
        <li>Install java.</li>
        <li>Setup Hadoop.</li>
    </ul>
    <p>Before proceeding further, we will need to copy the Public DNS of the created instance. You can find it under the Instance summary tab.</p>

    <h3>Accessing the Instance with Shell access</h3>
    <p>In the MobaXterm, change the home directory to where your key pair is downloaded.</p>
    <p>Then in the local terminal, you can connect to the instance with SSH using the following command:</p>
    <pre><code>ssh -i "hadoop.pem" ubuntu@ec2-54-197-203-5.compute-1.amazonaws.com</code></pre>
    <p>Then the terminal will be displayed something like this:</p>
    <p>And you will be accessing as an ubuntu user.</p>

    <h3>Common Configuration</h3>
    <p>In the following section, we will discuss the common configuration to perform for both the namenode and the datanode.</p>
    <p><strong>Update the Instance</strong></p>
    <pre><code>sudo apt-get update && sudo apt-get -y dist-upgrade</code></pre>
    <p><strong>Reboot the instance</strong></p>
    <p>Reboot from the EC2 service.</p>
    <p>Note: you will need to connect with ssh once again after the reboot is performed.</p>

    <h3>Install JAVA</h3>
    <pre><code>sudo apt-get -y install openjdk-8-jdk-headless</code></pre>

    <h3>Install Apache Hadoop</h3>
    <p>Install Apache Hadoop 2.7.3 on the instances. Install it under the server directory. Link to download can be found in the archive section.</p>
    <pre><code>mkdir server
cd server
wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
tar xvzf hadoop-2.7.3.tar.gz</code></pre>
    <p>View the extracted file using <code>ls</code> command</p>

    <h3>Setup JAVA_Home</h3>
    <pre><code>nano cd  ~/server/hadoop-2.7.3/etc/hadoop/hadoop-env.sh</code></pre>
    <p>Change the default with:</p>
    <pre><code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</code></pre>
    <p>Note: There are various ways to set up the environment variables. You can follow any practice you are comfortable with. Also, you can view this link for the generally practiced method to set up Hadoop Environment.</p>

    <h3>Update core_site.xml</h3>
    <p>The xml file /server/hadoop-2.7.3/etc/hadoop/core-site.xml after the configuration should look like this:</p>
    <pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;&lt;nnode&gt;:9000&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
    <p>Here <code>&lt;nnode&gt;</code> is the public DNS (or ec2-54-197-203-5.compute-1.amazonaws.com in my case).</p>

    <h3>Create Data dir</h3>
    <p>HDFS needs the data directory to be present on each node: 1 name node and 3 data nodes. Create this directory as shown and change ownership to user ubuntu.</p>
    <pre><code>sudo mkdir -p /usr/local/hadoop/hdfs/data
sudo chown -R ubuntu:ubuntu /usr/local/hadoop/hdfs/data</code></pre>

    <h3>Setup SSH</h3>
    <p>We need password-less SSH between the name nodes and the data nodes. Let us create a public-private key pair for this purpose on this instance.</p>
    <pre><code>ssh-keygen</code></pre>
    <p>Use the default (/home/ubuntu/.ssh/id_ed25519) for the key location and hit enter for an empty passphrase.</p>
    <p>Note: your key can be different like id_rsa. For that, you can view the files in .ssh folder.</p>
    <p>Then copy the pub keys from id_ed25519.pub and append it to the authorized_keys</p>
    <pre><code>cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></pre>
    <p>This is just for educational purposes, you are not advised to expose your private and public keys.</p>
    <p>To verify, enter <code>ssh localhost</code>. Then you can see something like this or else you will get an error.</p>

    <h3>Setup HDFS Properties</h3>
    <p>Edit the following file: /server/hadoop-2.7.3/etc/hadoop/hdfs-site.xml so that the final configuration should look like this:</p>
    <pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;3&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:///usr/local/hadoop/hdfs/data/namenode&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;file:///usr/local/hadoop/hdfs/data/datanode&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>

    <h3>Update the MapRed Properties</h3>
    <p>The mapred-site.xml needs to be edited. Edit the following file: /server/hadoop-2.7.3/etc/hadoop/mapred-site.xml so that the final configuration should look like this:</p>
    <pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>

    <h3>Update Yarn Properties</h3>
    <p>Edit the following file: /server/hadoop-2.7.3/etc/hadoop/yarn-site.xml so that the final configuration should look like this:</p>
    <pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>

    <h3>Copy and Mirror the Instance</h3>
    <p>Now that our common setup is ready, we can create a mirror for our instance to use as a data node. For this, stop the instance. After the instance is stopped, the EC2 console will show the option for AMI.</p>
    <p>Click the instance created. Then click on the image and create an image option. Give the instance an Image name. Then you can view the image created in the AMI section. Once the image is created, create instances (3) from that image.</p>

    <h3>Setting up the NameNode</h3>
    <p>To set up the NameNode, do the following:</p>
    <pre><code>cd ~/server/hadoop-2.7.3/sbin
./hadoop-daemon.sh --script hdfs start namenode</code></pre>
    <p>Format the name node:</p>
    <pre><code>cd ~/server/hadoop-2.7.3/bin
hdfs namenode -format</code></pre>
    <p>Start the NameNode:</p>
    <pre><code>cd ~/server/hadoop-2.7.3/sbin
./start-dfs.sh</code></pre>

    <h3>Setting up the DataNode</h3>
    <p>On all the DataNodes, do the following:</p>
    <pre><code>cd ~/server/hadoop-2.7.3/sbin
./hadoop-daemon.sh --script hdfs start datanode</code></pre>

    <h3>Verification</h3>
    <p>You can verify the Hadoop cluster setup by accessing the web interface of the NameNode. Open a browser and go to <code>http://&lt;Public_DNS&gt;:50070</code>. You should see the NameNode web interface showing the status of the cluster.</p>

    <h2>Conclusion</h2>
    <p>Congratulations! You have successfully set up a Hadoop cluster on AWS with a single NameNode and three DataNodes. This setup can be used for processing large datasets in a distributed manner, leveraging the power of Hadoop on the cloud.</p>
</body>
</html>
